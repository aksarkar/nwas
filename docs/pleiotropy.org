#+TITLE: Modeling pleiotropic effects
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:t
#+OPTIONS: todo:t |:t
#+CREATOR: Emacs 25.1.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Introduction

  Suppose we generated data from both mediated and unmediated effects:

  \[ y^1 = x^1 w v + x^1 u + e \]

  If we require that \(w\) explains the gene expression observations, then can
  we accurately estimate \(v\) even if \(u\) is correlated with \(v\)?

  It is biologically plausible for a variant to have both a mediated and an
  unmediated effect?

  Do we have any documented examples of this?

  Can we estimate how much variance is explained by unmediated effects?

  Can we eliminate this possibility by considering a large enough region of the
  genome (enough genes)? Can we eliminate it by regressing out the rest of the
  genome?

* Simulation setup

  - Generate random genotypes in linkage equilibrium

  - Generate Gaussian gene expression from a linear model (PVE = 0.5)

  #+BEGIN_LaTeX
  \[ w_j \mid \text{causal} ~ N(0, 1) \]
  \[ g^0_{ik} \mid \text{causal} ~ x^0 w + e \]
  #+END_LaTeX

  - Generate non-causal gene expression by sampling from a Gaussian with scale
    equal to the simulated expression phenotypic variance.

  \[ g_{ik} \mid \text{not causal} ~ N(0, V[g \mid \causal]) \]

  - Generate GWAS expression from the same linear model

  \[ g^1_{ik} \mid \text{causal} ~ x^1 w + e \]

  - Generate pleiotropic unmediated effect (same variant, independent
    effect on phenotype). Scale to get the desired PVE.

  \[ u \mid \text{causal} = N(0, 1) \]

  - Generate Gaussian phenotypes using the unmediated effect and adding noise
    to get the desired PVE.

  \[ y^1 \mid N(x^1 u, \sigma^2) \]

  #+BEGIN_SRC python :tangle pleiotropy.py :exports none
    import edward as ed
    import numpy
    import nwas
    import scipy.special
    import tensorflow as tf

    from edward.models import *
    from nwas.models import *

    p = 100  # Number of SNPs
    m = 10  # Number of genes
    n_ref = 500
    n_gwas = 10000
    pve_g = 0.5
    pve_y = 0.01  # Proportion of phenotypic variance explained by unmediated effects

    with nwas.simulation.simulation(p, pve_g, [(3, 1)], 0) as s:
        x_ref, g_ref = s.sample_gaussian(n=n_ref)
        g_noise = s.random.normal(scale=numpy.sqrt(s.pheno_var), size=(n_ref, m - 1))
        g_ref = numpy.hstack((g_ref.reshape(-1, 1), g_noise))
        x_gwas, g_gwas = s.sample_gaussian(n=n_gwas)

        # Pleiotropic (unmediated) effects
        true_u = numpy.zeros(p)
        causal = s.theta != 0
        true_u[causal] = numpy.random.normal(size=causal.sum())
        y_gwas = x_gwas.dot(true_u)

        # Add residual
        y_gwas += s.random.normal(scale=numpy.sqrt(y_gwas.var() * (1 / pve_y - 1)), size=n_gwas)

        # Center
        y_gwas -= y_gwas.mean()

        x_ref = x_ref.astype('float32')
        g_ref = g_ref.astype('float32')
        x_gwas = x_gwas.astype('float32')
        y_gwas = y_gwas.reshape(-1, 1).astype('float32')
  #+END_SRC

* Joint model

  #+BEGIN_SRC python :tangle pleiotropy.py
    ed.set_seed(0)

    # Data
    x0 = tf.placeholder(tf.float32)
    x1 = tf.placeholder(tf.float32)

    # eQTL effects
    logodds_w = Normal(loc=tf.constant(-10.0), scale=tf.ones(1))
    scale_w = Normal(loc=tf.zeros(1), scale=tf.ones(1))
    w = SpikeSlab(logodds=logodds_w, loc=tf.zeros([p, m]), scale=scale_w)
    # This is a dummy which gets swapped out in the inference
    eta0 = LocalReparameterization(Normal(tf.matmul(x0, w), 1.0))
    g0 = NormalWithSoftplusScale(
        loc=eta0, scale=tf.Variable(tf.random_normal([1])))

    # Mediated gene effects
    logodds_v = Normal(loc=tf.constant(-10.0), scale=tf.ones(1))
    scale_v = Normal(loc=tf.zeros(1), scale=tf.ones(1))
    v = SpikeSlab(logodds=logodds_v, loc=tf.zeros([m, 1]), scale=scale_v)
    eta1m = LocalReparameterization(Normal(tf.matmul(tf.matmul(x1, w), v), 1.0))

    # Unmediated effects
    logodds_u = Normal(loc=tf.constant(-10.0), scale=tf.ones(1))
    scale_u = Normal(loc=tf.zeros(1), scale=tf.ones(1))
    u = SpikeSlab(logodds=logodds_u, loc=tf.zeros([p, 1]), scale=scale_u)
    eta1u = LocalReparameterization(Normal(tf.matmul(x1, u), 1.0))

    y1 = NormalWithSoftplusScale(loc=eta1m + eta1u, scale=tf.Variable(0.0))
  #+END_SRC

* Variational approximation

  #+BEGIN_SRC python :tangle pleiotropy.py
    q_logodds_w = Normal(loc=tf.Variable(tf.random_normal([1])),
                         scale=tf.Variable(tf.random_normal([1])))
    q_logodds_v = Normal(loc=tf.Variable(tf.random_normal([1])),
                         scale=tf.Variable(tf.random_normal([1])))
    q_logodds_u = Normal(loc=tf.Variable(tf.random_normal([1])),
                         scale=tf.Variable(tf.random_normal([1])))
    q_scale_w = Normal(loc=tf.Variable(tf.random_normal([1])),
                       scale=tf.Variable(tf.random_normal([1])))
    q_scale_v = Normal(loc=tf.Variable(tf.random_normal([1])),
                       scale=tf.Variable(tf.random_normal([1])))
    q_scale_u = Normal(loc=tf.Variable(tf.random_normal([1])),
                       scale=tf.Variable(tf.random_normal([1])))

    import scipy.linalg
    initial_w, *_ = scipy.linalg.lstsq(x_ref, g_ref)

    q_w = SpikeSlab(logodds=tf.Variable(tf.zeros([p, m])),
                    loc=tf.Variable(initial_w.astype('float32')),
                    scale=tf.Variable(tf.zeros([p, m])))
    q_eta0 = LocalReparameterization(
        Normal(loc=tf.matmul(x0, q_w.mean()),
               scale=tf.sqrt(tf.matmul(tf.square(x0), q_w.variance()))))
  #+END_SRC

  We need to do some work to get the reparameterized distribution \(q(X w
  v)\). As previously derived (Brown 1977), if b, c are \(n\)-dimensional
  Gaussian then:

  \[ E[b' c] = E[b]' E[c] \]

  \[ V[b' c] = E[b]' Cov(c, c) E[b] + E[c]' Cov(b, b) + E[c]  + \Tr(Cov(b, b) Cov(c, c)) \]

  Here, we need moments of a stochastic matrix-vector product. However, under the
  variational approximation, all of the elements are independent, simplifying the
  derivation. Let \(\eta = X w\). Then considering each row \(\eta_i\), we can
  simply apply the above result to get:

  \[ E_q[\eta_i v] = E_q[\eta_i] E_q[v] \]

  \[ V_q[\eta_i v] = E_q[\eta_i] \diag(V_q[v]) E_q[\eta_i]' + E_q[v]' \diag(V_q[\eta_i]) E_q[v] + V_q[\eta_i]' V_q[v] \]

  #+BEGIN_SRC python :tangle pleiotropy.py
    q_v = SpikeSlab(logodds=tf.Variable(tf.zeros([m, 1])),
                    loc=tf.Variable(tf.random_normal([m, 1])),
                    scale=tf.Variable(tf.zeros([m, 1])))
    # Conviently keep the necessary mean and variance around
    q_eta1 = Normal(loc=tf.matmul(x1, q_w.mean()),
                    scale=tf.sqrt(tf.matmul(tf.square(x1), q_w.variance())))
    var = (tf.reduce_sum(tf.square(q_eta1.mean()) *
                         tf.transpose(q_v.variance()), axis=1, keep_dims=True) +
           tf.reduce_sum(tf.transpose(tf.square(q_v.mean())) *
                         q_eta1.variance(), axis=1, keep_dims=True) +
           tf.matmul(q_eta1.variance(), q_v.variance()))
    q_eta1m = LocalReparameterization(
        Normal(loc=tf.matmul(tf.matmul(x1, q_w.mean()), q_v.mean()),
               scale=tf.sqrt(var)))

    q_u = SpikeSlab(logodds=tf.Variable(tf.zeros([p, 1])),
                    loc=tf.Variable(tf.zeros([p, 1])),
                    scale=tf.Variable(tf.zeros([p, 1])))
    q_eta1u = LocalReparameterization(
        Normal(loc=tf.matmul(x1, q_u),
               scale=tf.sqrt(tf.matmul(tf.square(x1), q_u.variance()))))
  #+END_SRC

* Model fitting

  #+BEGIN_SRC python :tangle pleiotropy.py
    inference = ed.ReparameterizationKLKLqp(
        latent_vars={
            logodds_w: q_logodds_w,
            logodds_v: q_logodds_v,
            logodds_u: q_logodds_u,
            scale_w: q_scale_w,
            scale_v: q_scale_v,
            scale_u: q_scale_u,
            w: q_w,
            v: q_v,
            eta0: q_eta0,
            eta1m: q_eta1m,
            eta1u: q_eta1u,
        },
        data={
            x0: x_ref,
            g0: g_ref,
            x1: x_gwas,
            y1: y_gwas,
        })
    inference.run(n_iter=2000, optimizer='rmsprop')
  #+END_SRC
* Model evaluation

  #+BEGIN_SRC python :tangle pleiotropy.py
    import matplotlib.gridspec
    import matplotlib.pyplot as plt

    sess = ed.get_session()
    est_w = sess.run(q_w.pip)
    est_v = sess.run(q_v.pip)
    est_u = sess.run(q_u.pip)

    plt.switch_backend('pdf')
    gs = matplotlib.gridspec.GridSpec(2, 2, width_ratios=[100, 1])
    norm = matplotlib.colors.NoNorm(0, 1)
    fig = plt.gcf()
    fig.set_size_inches(8, 2)
    plt.clf()
    plt.subplot(gs[0])
    plt.imshow(est_w.T, cmap='Greys', norm=norm)
    ax = plt.gca()
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    ax.set_xlabel('')
    ax.set_ylabel('Genes')

    plt.subplot(gs[1])
    plt.imshow(est_v, cmap='Greys', norm=norm)
    ax = plt.gca()
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_xticklabels([])
    ax.set_yticklabels([])

    plt.subplot(gs[2])
    plt.imshow(est_u.T, cmap='Greys', norm=norm)
    ax = plt.gca()
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    ax.set_xlabel('Variants')

    plt.savefig('coefficients-pleiotropy')
    plt.close()
  #+END_SRC

file:coefficients-pleiotropy.pdf
* Limitations

  \(u\) correlated with \(v\) is the well studied pleiotropy problem
  in Mendelian randomization.

  Unfortunately, prior work (by others, and us!) on this problem suggests it
  can't be solved in one model. Starting from the Mendelian randomization
  perspective: if the goal is to estimate the causal effect \(v\), then we have
  to first estimate \(w\), then estimate \(v\) fixing \(X w\).

  Now, in the case where \(u\) is correlated with \(v\), this method is biased
  and requires correction. The relevant method is [[https://www.ncbi.nlm.nih.gov/pubmed/26050253][MR-Egger]], which has also been
  extended to [[https://arxiv.org/abs/1708.00272][multivariate case]]. The key idea is to fit weighted least squares,
  where the weights come from the standard error of the direct associations.

  \[ (w v + u) = w \tilde{v} + b \]

  Open questions:
 
  1. If instruments are correlated (thinking about strong LD within a locus),
     what breaks in MR-Egger?
  2. If we replace OLS with a posterior mean assuming the spike-and-slab
     prior, what happens?
  3. The key assumption (INSIDE) is that \(Cov(w v, u) = 0\). Is this plausible
     for the case of /cis/-regulatory variants within a single locus?
  4. The multivariate extension still requires an independence assumption
     between the different mediators. If we fit it using SSB, what happens?

  To make causal claims (about mediation), we further need to remove
  /trans/-effects and reverse causal effects on gene expression.

  We can do the first using half-sibling regression: regress observed genes
  expression against control gene expression, where control genes are on other
  chromosomes.

  We can do the second using a random effects approach. Suppose we regress gene
  expression against both genotype and phenotype, assuming a linear mixed
  model. Treat genotype effects as random by building a kernel matrix the rest
  of the genome, and treat expression effects as fixed (estimated).

  This could be done using BSLMM, or variational BSLMM as hinted by Peter.

  This still assumes gene expression is Gaussian. It should be trivial to build
  a negative binomial observation model over a latent linear model using
  Edward.
