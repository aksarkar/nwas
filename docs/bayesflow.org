#+TITLE: Spike-and-slab regression in Bayesflow
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.1 (Org mode 9.1.1)

#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t

* Setup

  #+BEGIN_SRC emacs-lisp
    (setq temporary-directory "/scratch/midway2/aksarkar"
          python-shell-prompt-regexp "In ")
  #+END_SRC

  #+RESULTS:
  : In 

  Run a remote ~ipython3~ kernel on ~midway2~, then connect to it locally using
  [[https://github.com/gregsexton/ob-ipython][ob-ipython]]. On the ~org-mode~ side, we need to name the connection file for the
  kernel in such a way that ~ob-ipython~ recognizes it.

  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "spikeslab")
  sbatch --partition=broadwl --mem=16G --time=36:00:00 --job-name=ipython3 --output=ipython3.out
  #!/bin/bash
  source activate nwas
  rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
  ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 39154189

  #+BEGIN_SRC ipython
    %matplotlib inline
    import os
    import matplotlib.pyplot as plt
    import numpy as np
    import nwas
    import tensorflow as tf

    from nwas.models.spikeslab import distribution_SpikeSlab

    ds = tf.contrib.distributions
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Simulate some data

  #+BEGIN_SRC ipython
    p = 1000
    n_train = 500
    n_validate = 500
    pve_y = 0.5

    with nwas.simulation.simulation(p, pve_y, [(100, 1)], 0) as s:
      x_train, y_train = s.sample_gaussian(n=n_train)
      x_validate, y_validate = s.sample_gaussian(n=n_validate)
      x_train = x_train.astype('float32')
      x_validate = x_validate.astype('float32')
      y_train = y_train.reshape(-1, 1).astype('float32')
      y_validate = y_validate.reshape(-1, 1).astype('float32')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Build up the stochastic graph  

  #+BEGIN_SRC ipython
    stoch_samples = 50
    graph = tf.Graph()

    with graph.as_default():
      x_ph = tf.placeholder(tf.float32)
      y_ph = tf.placeholder(tf.float32)

      with tf.variable_scope('q', initializer=tf.random_normal_initializer):
        with tf.variable_scope('residual_scale'):
          q_residual_scale = ds.Normal(loc=tf.get_variable('loc', [1]),
                                       scale=tf.get_variable('scale', [1]))
        with tf.variable_scope('logodds'):
          q_logodds = ds.Normal(loc=tf.get_variable('loc', [1]),
                                scale=tf.get_variable('scale', [1]))
        with tf.variable_scope('scale'):
          q_scale = ds.Normal(loc=tf.get_variable('loc', [1]),
                              scale=tf.get_variable('scale', [1]))
        with tf.variable_scope('theta'):
          q_theta = distribution_SpikeSlab(
            logodds=tf.get_variable('logodds', [p, 1]),
            loc=tf.get_variable('loc', [p, 1]),
            scale=tf.get_variable('scale', [p, 1]))

      p_residual_scale = ds.Normal(loc=tf.zeros([1]), scale=tf.constant(10.), name='p_residual')
      p_logodds = ds.Normal(loc=tf.constant(-5.), scale=tf.ones([1]), name='p_logodds')
      p_scale = ds.Normal(loc=tf.zeros([1]), scale=tf.ones([1]), name='p_scale')
      # This has to be defined after the variational part because variational
      # surrogates for hyperparameters appear in the KL term for this distribution
      p_theta = distribution_SpikeSlab(logodds=q_logodds._loc,
                                       loc=tf.zeros([p, 1]),
                                       scale=q_scale._loc,
                                       name='p_theta')

      approx = {
        q_residual_scale: p_residual_scale,
        q_logodds: p_logodds,
        q_scale: p_scale,
        q_theta: p_theta,
      }

      noise = tf.random_normal([stoch_samples, 2])
      eta = ds.Normal(loc=tf.matmul(x_ph, q_theta.mean()),
                      scale=tf.sqrt(tf.matmul(tf.square(x_ph), q_theta.variance())))
      llik = ds.NormalWithSoftplusScale(loc=eta._loc + noise[:, 0] * eta._scale,
                                        scale=q_residual_scale._loc + noise[:, 1] * q_residual_scale._scale)
      llik = tf.reduce_mean(tf.reduce_sum(llik.log_prob(y_ph), axis=0))
      elbo = llik
      for q_, p_ in approx.items():
        elbo -= ds.kl_divergence(q_, p_)
      optimizer = tf.train.RMSPropOptimizer(learning_rate=2.5e-3)
      train = optimizer.minimize(-elbo)

      # GLM coefficient of determination
      R = 1 - tf.reduce_sum(tf.square(y_ph - eta.mean())) / tf.reduce_sum(tf.square(y_ph - tf.reduce_mean(y_ph)))

      grads = tf.gradients(elbo, tf.trainable_variables())
      opt = [
        tf.nn.sigmoid(q_theta._logodds),
        q_theta.mean(),
        q_logodds.mean(),
        q_scale.mean(),
        q_residual_scale.mean(),
      ]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Optimize the variational objective  

  #+BEGIN_SRC ipython
    sv = tf.train.Supervisor(
      graph=graph,
      logdir=os.path.join(os.getenv('SCRATCH'), 'spike-slab-model'))
    with sv.managed_session() as sess:
      for i in range(4000):
        if sv.should_stop():
          break
        _, elbo_val = sess.run([train, elbo],
                               feed_dict={x_ph: x_train, y_ph: y_train})
        if np.isnan(elbo_val):
          raise tf.train.NanLossDuringTrainingError
        if not i % 100:
          print(i, elbo_val)
      training_score = sess.run(R, {x_ph: x_train, y_ph: y_train})
      validation_score = sess.run(R, {x_ph: x_validate, y_ph: y_validate})
      final_grads = sess.run(grads, {x_ph: x_train, y_ph: y_train})
      final_opt = sess.run(opt)
    training_score, validation_score
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : (0.76364934, -0.047796249)
  :END:

* Plot the fit

  #+BEGIN_SRC ipython
    plt.clf()
    q = np.logical_or(s.theta != 0, opt[0].ravel() > 0.1)
    fig, ax = plt.subplots(3, 1)
    fig.set_size_inches(6, 8)
    ax[0].bar(np.arange(np.sum(q)), s.theta[q])
    ax[0].set_ylabel('True effect size')
    ax[1].bar(np.arange(np.sum(q)), opt[1].ravel()[q])
    ax[1].set_ylabel('Estimated effect size')
    ax[2].bar(np.arange(np.sum(q)), opt[0].ravel()[q])
    ax[2].set_ylabel('Posterior inclusion probability')
    ax[2].set_xlabel('True and false positive variants')
    plt.savefig('fit.png')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:./obipy-resources/19307LHc.png]]
  :END:
