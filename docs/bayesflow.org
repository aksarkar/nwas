#+TITLE: Spike-and-slab regression in Bayesflow
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.1 (Org mode 9.1.1)

#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t

* Setup

  #+BEGIN_SRC emacs-lisp
    (setq temporary-directory "/scratch/midway2/aksarkar"
          python-shell-prompt-regexp "In ")
  #+END_SRC

  #+RESULTS:
  : In 

  Run a remote ~ipython3~ kernel on ~midway2~, then connect to it locally using
  [[https://github.com/gregsexton/ob-ipython][ob-ipython]]. On the ~org-mode~ side, we need to name the connection file for the
  kernel in such a way that ~ob-ipython~ recognizes it.

  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "spikeslab")
  sbatch --partition=broadwl --mem=16G --time=36:00:00 --job-name=ipython3 --output=ipython3.out
  #!/bin/bash
  source activate nwas
  rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
  ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 39295543

  #+BEGIN_SRC ipython
    %matplotlib inline
    import os
    import matplotlib.pyplot as plt
    import numpy as np
    import nwas
    import tensorflow as tf

    from nwas.models.spikeslab import distribution_SpikeSlab

    ds = tf.contrib.distributions
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Simulate some data

  #+BEGIN_SRC ipython
    p = 1000
    n_train = 500
    n_validate = 500
    pve_y = 0.5

    with nwas.simulation.simulation(p, pve_y, [(100, 1)], 0) as s:
      x_train, y_train = s.sample_gaussian(n=n_train)
      x_validate, y_validate = s.sample_gaussian(n=n_validate)
      x_train = x_train.astype('float32')
      x_validate = x_validate.astype('float32')
      y_train = y_train.reshape(-1, 1).astype('float32')
      y_validate = y_validate.reshape(-1, 1).astype('float32')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Build up the stochastic graph  

  #+BEGIN_SRC ipython
    stoch_samples = 50
    graph = tf.Graph()

    with graph.as_default():
      x_ph = tf.placeholder(tf.float32)
      y_ph = tf.placeholder(tf.float32)

      with tf.variable_scope('q', initializer=tf.random_normal_initializer):
        with tf.variable_scope('residual'):
          q_residual_scale = ds.Normal(loc=tf.get_variable('loc', [1]),
                                       scale=tf.get_variable('scale', [1]))
        with tf.variable_scope('logodds'):
          q_logodds = ds.Normal(loc=tf.get_variable('loc', [1]),
                                scale=tf.get_variable('scale', [1]))
        with tf.variable_scope('scale'):
          q_scale = ds.Normal(loc=tf.get_variable('loc', [1]),
                              scale=tf.get_variable('scale', [1]))
        with tf.variable_scope('theta'):
          q_theta = distribution_SpikeSlab(
            logodds=tf.get_variable('logodds', [p, 1]),
            loc=tf.get_variable('loc', [p, 1]),
            scale=tf.get_variable('scale', [p, 1]))

      p_residual_scale = ds.Normal(loc=tf.zeros([1]), scale=tf.constant(10.), name='p_residual')
      p_logodds = ds.Normal(loc=tf.constant(-5.), scale=tf.ones([1]), name='p_logodds')
      p_scale = ds.Normal(loc=tf.zeros([1]), scale=tf.ones([1]), name='p_scale')
      # This has to be defined after the variational part because variational
      # surrogates for hyperparameters appear in the KL term for this distribution
      p_theta = distribution_SpikeSlab(logodds=q_logodds._loc,
                                       loc=tf.zeros([p, 1]),
                                       scale=q_scale._loc,
                                       name='p_theta')

      approx = {
        q_residual_scale: p_residual_scale,
        q_logodds: p_logodds,
        q_scale: p_scale,
        q_theta: p_theta,
      }

      noise = tf.random_normal([stoch_samples, 2])
      eta = ds.Normal(loc=tf.matmul(x_ph, q_theta.mean()),
                      scale=tf.sqrt(tf.matmul(tf.square(x_ph), q_theta.variance())))
      llik = ds.NormalWithSoftplusScale(loc=eta._loc + noise[:, 0] * eta._scale,
                                        scale=q_residual_scale._loc + noise[:, 1] * q_residual_scale._scale)
      llik = tf.reduce_mean(tf.reduce_sum(llik.log_prob(y_ph), axis=0))
      elbo = llik
      for q_, p_ in approx.items():
        elbo -= ds.kl_divergence(q_, p_)
      optimizer = tf.train.RMSPropOptimizer(learning_rate=2.5e-3)
      train = optimizer.minimize(-elbo)

      # GLM coefficient of determination
      R = 1 - tf.reduce_sum(tf.square(y_ph - eta.mean())) / tf.reduce_sum(tf.square(y_ph - tf.reduce_mean(y_ph)))

      grads = tf.gradients(elbo, tf.trainable_variables())
      opt = [
        tf.nn.sigmoid(q_theta._logodds),
        q_theta.mean(),
        q_logodds.mean(),
        q_scale.mean(),
        tf.nn.softplus(q_residual_scale.mean()),
      ]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Port the Theano version

  #+BEGIN_SRC ipython
    graph = tf.Graph()

    with graph.as_default():
      x_ph = tf.placeholder(tf.float32)
      y_ph = tf.placeholder(tf.float32)

      with tf.variable_scope('model', initializer=tf.zeros_initializer):
        # residual
        q_log_prec_mean = tf.get_variable('q_log_prec_mean', shape=[1])
        q_log_prec_log_prec = tf.get_variable('q_log_prec_log_prec', shape=[1])
        q_log_prec_prec = 1e-6 + tf.nn.softplus(q_log_prec_log_prec)
        q_log_prec_std = tf.sqrt(tf.reciprocal(q_log_prec_prec))

        q_logodds_mean = tf.get_variable('q_logodds_mean', initializer=tf.constant([-10.]))
        q_logodds_log_prec = tf.get_variable('q_logodds_log_prec', shape=[1])
        q_logodds_prec = 1e-6 + tf.nn.softplus(q_logodds_log_prec)
        # In [685]: np.log(np.finfo('float32').resolution)
        # Out[693]: -13.815511
        pi = tf.nn.sigmoid(tf.clip_by_value(q_logodds_mean, -13, 13))

        q_scale_mean = tf.get_variable('q_scale_mean', shape=[1])
        q_scale_log_prec = tf.get_variable('q_scale_log_prec', shape=[1])
        q_scale_prec = 1e-6 + tf.nn.softplus(q_scale_log_prec)
        tau = tf.nn.softplus(q_scale_mean)

        q_logit_z = tf.get_variable('q_logit_z', shape=[p, 1])
        q_z = tf.nn.sigmoid(tf.clip_by_value(q_logit_z, -13, 13))

        q_theta_mean = tf.get_variable('q_theta_mean', shape=[p, 1])
        q_theta_log_prec = tf.get_variable('q_theta_log_prec', shape=[p, 1])
        q_theta_prec = 1e-6 + tf.nn.softplus(q_theta_log_prec)

      theta_posterior_mean = q_z * q_theta_mean
      theta_posterior_var = q_z / q_theta_prec + q_z * (1 - q_z) * tf.square(q_theta_mean)
      eta_mean = tf.matmul(x_ph, theta_posterior_mean)
      eta_std = tf.sqrt(tf.matmul(tf.square(x_ph), theta_posterior_var))

      noise = tf.random_normal([50, 2])
      eta = eta_mean + noise[:,0] * eta_std
      phi = tf.nn.softplus(q_log_prec_mean + noise[:,1] * q_log_prec_std)

      llik = -.5 * tf.reduce_mean(tf.reduce_sum(-tf.log(phi) + tf.square(y_ph - eta) * phi, axis=0))
      kl_z = tf.reduce_sum(q_z * tf.log(q_z / pi) + (1 - q_z) * tf.log((1 - q_z) / (1 - pi)))
      kl_theta = tf.reduce_sum(q_z * .5 * (1 - tf.log(tau) + tf.log(q_theta_prec) + tau * (tf.square(q_theta_mean) + 1 / q_theta_prec)))
      kl_logodds = .5 * tf.reduce_sum(1 + tf.log(q_logodds_prec) + (tf.square(q_logodds_mean) + 1 / q_logodds_prec))
      kl_scale = .5 * tf.reduce_sum(1 + tf.log(q_scale_prec) + (tf.square(q_scale_mean) + 1 / q_scale_prec))
      kl_log_prec = .5 * tf.reduce_sum(1 + tf.log(q_log_prec_prec) + (tf.square(q_log_prec_mean) + 1 / q_log_prec_prec))
      elbo = llik - kl_z - kl_theta - kl_logodds - kl_scale - kl_log_prec

      optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2)
      train = optimizer.minimize(-elbo)

      # GLM coefficient of determination
      R = 1 - tf.reduce_sum(tf.square(y_ph - eta_mean)) / tf.reduce_sum(tf.square(y_ph - tf.reduce_mean(y_ph)))

      opt = [
        q_z,
        theta_posterior_mean,
        pi,
        tau,
      ]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

* Optimize the variational objective  

  #+BEGIN_SRC ipython
    sv = tf.train.Supervisor(
      graph=graph,
      logdir=os.path.join(os.getenv('SCRATCH'), 'spike-slab-model'))
    with sv.managed_session() as sess:
      for i in range(4000):
        if sv.should_stop():
          break
        _, *trace = sess.run([train, elbo, llik, kl_z, kl_theta, kl_logodds, kl_scale, kl_log_prec],
                             feed_dict={x_ph: x_train, y_ph: y_train})
        if np.isnan(elbo_val):
          raise tf.train.NanLossDuringTrainingError
        if not i % 100:
          print(i, *trace)
      sv.saver.save(sess, sv.save_path)
      training_score = sess.run(R, {x_ph: x_train, y_ph: y_train})
      validation_score = sess.run(R, {x_ph: x_validate, y_ph: y_validate})
      final_opt = sess.run(opt)
    training_score, validation_score
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : (0.44175905, 0.26670796)
  :END:

* Plot the fit

  #+BEGIN_SRC ipython :ipyfile fit.svg
    plt.clf()
    q = np.logical_or(s.theta != 0, final_opt[0].ravel() > 0.1)
    fig, ax = plt.subplots(3, 1)
    fig.set_size_inches(6, 8)
    ax[0].bar(np.arange(np.sum(q)), s.theta[q])
    ax[0].set_ylabel('True effect size')
    ax[1].bar(np.arange(np.sum(q)), final_opt[1].ravel()[q])
    ax[1].set_ylabel('Estimated effect size')
    ax[2].bar(np.arange(np.sum(q)), final_opt[0].ravel()[q])
    ax[2].set_ylabel('Posterior inclusion probability')
    ax[2].set_xlabel('True and false positive variants')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : <matplotlib.text.Text at 0x2b9238e70e10>
  [[file:fit.svg]]
  :END:
