#+TITLE: Linear regression with spike and slab prior
#+SETUPFILE: /home/aksarkar/.emacs.d/org-templates/setup-hotlink.org

#+BEGIN_SRC ipython
  import edward as ed
  import matplotlib.pyplot as plt
  import numpy
  import nwas
  import scipy.special
  import tensorflow as tf

  from edward.models import *
  from nwas.models import *
#+END_SRC

Simulate some data:

#+BEGIN_SRC ipython
  p = 10000
  n_train = 7500
  n_validate = 1000
  pve_y = 0.5

  with nwas.simulation.simulation(p, pve_y, [(100, 1)], 0) as s:
      true_w = s.theta.reshape(-1, 1)
      x_train, y_train = s.sample_gaussian(n=n_train)
      x_validate, y_validate = s.sample_gaussian(n=n_validate)
      x_train = x_train.astype('float32')
      y_train = y_train.reshape(-1, 1).astype('float32')
      x_validate = x_validate.astype('float32')
      y_validate = y_validate.reshape(-1, 1).astype('float32')
#+END_SRC

Set up the model:

#+BEGIN_SRC ipython
  x = tf.placeholder(tf.float32)
  logodds = Normal(loc=-numpy.log(p).astype('float32'), scale=tf.ones(1))
  scale = Normal(loc=tf.zeros([1]), scale=tf.ones([1]))
  w = SpikeSlab(logodds=logodds,
                loc=tf.zeros([p, 1]),
                scale=scale,
  )
  # This is a dummy which gets swapped out in inference. It only needs to have
  # the correct shape
  eta = LocalReparameterization(ed.models.Normal(tf.matmul(x, w), 1.0))
  y = NormalWithSoftplusScale(loc=eta, scale=tf.Variable(tf.zeros([1])))
#+END_SRC

Set up the variational approximation:

#+BEGIN_SRC ipython
  q_logodds = Normal(loc=tf.Variable(tf.random_normal([1])),
                     scale=tf.Variable(tf.random_normal([1])))
  q_scale = Normal(loc=tf.Variable(tf.random_normal([1])),
                   scale=tf.Variable(tf.random_normal([1])))
  q_w = SpikeSlab(
      logodds=tf.Variable(tf.zeros([p, 1])),
      loc=tf.Variable(tf.zeros([p, 1])),
      scale=tf.Variable(tf.zeros([p, 1])))
  q_eta = LocalReparameterization(
      ed.models.Normal(loc=tf.matmul(x, q_w.mean()),
      scale=tf.sqrt(tf.matmul(tf.square(x), q_w.variance()))))
#+END_SRC

Fit the approximation:

#+BEGIN_SRC ipython
  inference = ed.ReparameterizationKLKLqp(
      latent_vars={
          logodds: q_logodds,
          scale: q_scale,
          w: q_w,
          eta: q_eta,
      },
      data={
          x: x_train,
          y: y_train,
      })

  inference.run(n_iter=1000, optimizer='rmsprop')
#+END_SRC

Plot the fit:

#+BEGIN_SRC ipython
  session = ed.get_session()
  pip = session.run(q_w.pip)
  est_w = session.run(q_w.mean())

  plt.switch_backend('pdf')
  q = numpy.logical_or(pip > 0.1, true_w != 0)
  nq = numpy.count_nonzero(q)
  fig, ax = plt.subplots(3, 1)
  fig.set_size_inches(6, 8)
  plt.xlabel('True and false positive variants')
  ax[0].bar(range(nq), true_w[q])
  ax[0].set_ylabel('True effect size')
  ax[1].bar(range(nq), est_w[q])
  ax[1].set_ylabel('Estimated effect size')
  ax[2].bar(range(nq), pip[q])
  ax[2].set_ylabel('PIP')
  plt.savefig('example')
  plt.close()
#+END_SRC

Check the fit. We can't do this using ~ed.copy~ because the copied nodes remain
in the graph (unlike the way it works in Theano), so we can't feed in data of
different shapes.

#+BEGIN_SRC ipython
  def correlation_score(y_true, y_pred):
      R = 1 - (tf.reduce_sum(tf.square(y_true - y_pred)) /
               tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true))))
      return ed.get_session().run(R)

  print('Training set score = {:.3f}'.format(
      correlation_score(y_train, session.run(q_eta.mean(), {x: x_train}))))
  print('Validation set score = {:.3f}'.format(
      correlation_score(y_validate, session.run(q_eta.mean(), {x: x_validate}))))
#+END_SRC
