#+TITLE: Fine mapping idea
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+OPTIONS: num:nil
#+SETUPFILE: /home/aksarkar/.local/src/org-html-themes/setup/theme-readtheorg.setup
#+PROPERTY: header-args:ipython+ :session kernel-aksarkar.json :results raw drawer :async t :exports both :eval never-export

* Proposed model

  Revise the notation, using Latin for the model and Greek for the variational
  approximation.

  \[ p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}) = N(\mathbf{y}; \mathbf{x} \mathbf{w}', v^{-1} \mathbf{I}) \]
  \[ \mathbf{w} = \sum_k z_{kj} b_{kj} \]

  \[ p(b_{kj} \mid z_k = 1, v, v_b) = N(b_{kj}; 0, v^{-1} v_b^{-1}) \]
  \[ p(b_{kj} \mid z_k = 0) = \delta(b_{kj}) \]

  \[ p(z_k \mid \mathbf{p}) = \mathrm{Multinomial}(z_k; 1, \mathbf{p}) \]

  \[ q(b_{kj} \mid z_{kj} = 1, \mu, \phi) = N(b_{kj}; \mu_{kj}, \phi_{kj}^{-1}) \]
  \[ q(b_{kj} \mid z_{kj} = 0, \mu, \phi) = \delta(b_{kj}) \]

  \[ q(z_k \mid \pi) = \mathrm{Multinomial}(z_k; 1, \mathbf{\pi}_k) \]

* Variational lower bound

  \[ \mathrm{ELBO} = \mathbb{E}_q[\ln p(\mathbf{y} \mid \mathbf{x}, \mathbf{w})] -
  KL(q(\mathbf{z}) \Vert p(\mathbf{z})) - KL(q(\mathbf{b} \mid \mathbf{z}) \Vert p(\mathbf{b} \mid \mathbf{z}))
  \]
  \[ \mathbb{E}_q[\ln p(\mathbf{y} \mid \mathbf{x}, \mathbf{w})] = -\frac{v}{2} \sum_i
  \left(y_i - \sum_j x_{ij} \mathbb{E}_q[w_j]\right)^2 - \sum_{i,j} x_{ij}^2 \mathbb{V}_q[w_j] \]

  \[ \mathbb{E}_q[w_j] = \sum_k \pi_{kj} \mu_{kj} \]
  \[ \mathbb{V}_q[w_j] = \sum_k \pi_{kj} \phi_{kj}^{-1} + \pi_{kj} (1 - \pi_{kj}) \mu_{kj}^2 \]

  \[ KL(q(\mathbf{z}) \Vert p(\mathbf{z})) = \sum_k E_q[\ln q(z_k) - \ln p(z_k)] \]
  \[ = \sum_k \left[\sum_j \pi_{kj} \ln \pi_{kj} - \sum_j \pi_{kj} \ln p_{j}\right] \]
  \[ = \sum_{j,k} \pi_{kj} \left( \ln \pi_{kj} - \ln p_{j} \right) \]

  \[ KL(q(\mathbf{b} \mid \mathbf{z}) \Vert p(\mathbf{b} \mid \mathbf{z})) =
  \frac{1}{2} \sum_{j,k} \pi_{kj} \left(1 + \ln (v v_b) - \ln \phi_{kj} + v v_b (\mu_{kj}^2 + \phi_{kj}^{-1}) \right)\]

* Setup

  #+NAME: srun
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "spikeslab")
  sbatch --partition=broadwl --mem=16G --time=36:00:00 --job-name=ipython3 --output=ipython3.out
  #!/bin/bash
  source activate nwas
  rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
  ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS: srun
  : Submitted batch job 39733997

  #+NAME: imports
  #+BEGIN_SRC ipython
    %matplotlib inline
    import edward as ed
    import matplotlib.pyplot as plt
    import nwas
    import numpy as np
    import pandas as pd
    import pyplink
    import tensorflow as tf
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  :END:

* Simulated data

  #+NAME: sim
  #+BEGIN_SRC ipython
    with nwas.simulation.simulation(p=1000, pve=0.5, annotation_params=[(10, 1)], seed=0) as s:
      x, y = s.sample_gaussian(n=500)
      x = x.astype('float32')
      y = y.reshape(-1, 1).astype('float32')
  #+END_SRC

  #+RESULTS: sim
  :RESULTS:
  :END:

* Coordinate ascent

  Between the old approximation and this approximation, the only difference is
  \(KL\left(q(z)\Vert p(z)\right)\), but it has similar form. This suggests
  that the same update would work.

  The real problem is that the optimization is now constrained to have \(\pi\)
  on the probability simplex, which isn't captured in the objective function.

  Normalizing \(\pi_k\) after each update does not increase the ELBO.

  #+BEGIN_SRC ipython
    def elbo(x, y, pip, mean, var, effect_var, residual_var):
      n, p = x.shape
      genetic_value_mean = np.dot(x, (pip * mean).sum(axis=0, keepdims=True).T)
      genetic_value_var = np.dot(np.square(x), (pip * var.T + pip * (1 - pip) * np.square(mean)).sum(axis=0, keepdims=True).T)
      llik = -.5 / residual_var * (np.square(y - genetic_value_mean) - genetic_value_var).sum()
      # Assume prior probability 1/p for each variant
      kl_z = (pip * (np.log(pip) + np.log(p))).sum()
      kl_b = .5 * (pip * (1 + np.log(effect_var * residual_var) - np.log(var.T) + (np.square(mean) + var.T) / (effect_var * residual_var))).sum()
      return llik, kl_z, kl_b

    def coordinate_ascent(x, y, effect_var, residual_var, l=5, num_epochs=200):
      n, p = x.shape
      pi = np.ones((p, 1))
      d = np.einsum('ij,ij->j', x, x).reshape(-1, 1)
      xy = x.T.dot(y)
      pip = np.zeros((l, p))
      mean = np.zeros((l, p))
      # Make sure everything is two dimensional to catch numpy broadcasting gotchas
      var = (effect_var * residual_var / (effect_var * d + 1)).reshape(-1, 1)
      eta = np.dot(x, (pip * mean).sum(axis=0, keepdims=True).T)
      elbo_ = None
      trace = []
      for epoch in range(num_epochs):
        for k in range(l):
          eta -= np.dot(x, (pip * mean)[k:k + 1].T)
          mean[k:k + 1] = (var / residual_var * (xy - x.T.dot(eta))).T
          pip[k:k + 1] = (pi * np.exp(.5 * (np.log(var / (effect_var * residual_var)) + np.square(mean[k:k + 1].T) / var))).T
          pip[k] /= pip[k].sum()
          eta += np.dot(x, (pip * mean)[k:k + 1].T)
        llik, kl_z, kl_b = elbo(x, y, pip, mean, var, effect_var, residual_var)
        update = llik - kl_z - kl_b
        trace.append([update, llik, kl_z, kl_b])
        elbo_ = update
      return {'pip': pip,
              'mean': pip * mean,
              'var': var,
              'elbo': elbo_,
              'trace': trace}
  #+END_SRC

  #+NAME: coordinate-ascent
  #+BEGIN_SRC ipython :results raw drawer
    opt = coordinate_ascent(x, y, effect_var=1, residual_var=s.residual_var, l=5, num_epochs=10)
    pd.DataFrame(opt['trace'], columns=['elbo', 'llik', 'kl_z', 'kl_b'])
  #+END_SRC

  #+RESULTS: coordinate-ascent
  :RESULTS:
  #+BEGIN_EXAMPLE
           elbo        llik       kl_z       kl_b
    0 -268.129734 -226.243188  25.352915  16.533631
    1 -268.576104 -226.869986  25.130829  16.575289
    2 -268.640320 -226.956885  25.105590  16.577845
    3 -268.635618 -226.929528  25.125407  16.580682
    4 -268.617950 -226.879634  25.153811  16.584505
    5 -268.593716 -226.816238  25.188131  16.589346
    6 -268.535093 -226.682496  25.255828  16.596769
    7 -268.360134 -226.292446  25.456290  16.611398
    8 -268.081829 -225.462570  25.979514  16.639745
    9 -268.019034 -224.800028  26.550398  16.668607
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython
    def plot_categorical_slab_fit(s, opt):
      plt.clf()
      q = np.logical_or(s.theta != 0, opt['pip'].sum(axis=0) > 0.1)
      fig, ax = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1, 1, opt['pip'].shape[0]]})
      fig.set_size_inches(6, 8)
      ax[0].bar(np.arange(np.sum(q)), s.theta[q])
      ax[0].set_xticks(np.arange(q.sum()))
      ax[0].set_ylabel('True effect size')
      ax[1].bar(np.arange(np.sum(q)), opt['mean'].sum(axis=0)[q])
      ax[1].set_ylabel('Estimated effect size')
      im = ax[2].imshow(opt['pip'][:,q], cmap='Reds', vmin=0, vmax=1)
      ax[2].set_yticks(np.arange(opt['pip'].shape[0]))
      ax[2].set_ylabel('Posterior inclusion probability')
      ax[2].set_xlabel('True and false positive variants')
      plt.colorbar(im, ax=ax[2], orientation='horizontal')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile coordinate-ascent-opt.png
    plot_categorical_slab_fit(s, opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:coordinate-ascent-opt.png]]
  :END:

  *TODO:* why is the log likelihood decreasing in the second round of updates?

  #+BEGIN_SRC ipython :ipyfile coordinate-ascent-1-iter.png
    opt = coordinate_ascent(x, y, l=5, effect_var=1, residual_var=s.residual_var, num_epochs=1)
    plot_categorical_slab_fit(s, opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:coordinate-ascent-1-iter.png]]
  :END:

* Harder simulation

  Use real LD from [[http://www.geuvadis.org/web/geuvadis/rnaseq-project][GEUVADIS genotypes]].

  #+NAME: plink
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "spikeslab") :async t
    srun --partition=broadwl --mem=8G plink --memory 8000 --vcf /project/compbio/geuvadis/genotypes/GEUVADIS.chr1.PH1PH2_465.IMPFRQFILT_BIALLELIC_PH.annotv2.genotypes.vcf.gz --make-bed --out 1
  #+END_SRC

  #+RESULTS: plink

  #+NAME: sim-geuvadis
  #+BEGIN_SRC ipython
    geuvadis_chr1 = pyplink.PyPlink(os.path.join(os.getenv('SCRATCH'), 'spikeslab', '1'))
    x = np.zeros((geuvadis_chr1.get_nb_samples(), s.p), dtype=np.float32)
    for i, (_, geno) in enumerate(geuvadis_chr1):
      if i >= x.shape[1]:
        break
      x[:,i] = geno.astype(np.float32)
    x = np.ma.masked_equal(x, -1)
  #+END_SRC

  #+RESULTS: sim-geuvadis
  :RESULTS:
  :END:
  
  #+NAME: ld
  #+BEGIN_SRC ipython :ipyfile ld.png
    def plot_ld(x):
      w = x - x.mean(axis=0)
      w /= 1e-8 + w.std(axis=0)
      corr = w.T.dot(w) / w.shape[0]
      plt.clf()
      plt.gcf().set_size_inches(8, 8)
      plt.imshow(np.triu(corr), cmap='RdBu_r', vmin=-1, vmax=1)
      cb = plt.colorbar()
      cb.ax.set_xlabel('Correlation')

    plot_ld(x)
  #+END_SRC

  #+RESULTS: ld
  :RESULTS:
  [[file:ld.png]]
  :END:

  Simulate a problem where causal variants almost certain to be in tight LD.

  #+BEGIN_SRC ipython
    s.estimate_mafs(x)
    annotation = np.zeros(s.p)
    annotation[600:610] = np.ones(10)
    s.load_annotations(annotation)
    s.sample_effects(pve=0.5, annotation_params=[(0, 1), (2, 1)], permute=True)
    x = (x - x.mean(axis=0)).filled(0)
    y = s.compute_liabilities(x).reshape(-1, 1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+NAME: coordinate-ascent-ld
  #+BEGIN_SRC ipython
    opt = coordinate_ascent(x, y, effect_var=1, residual_var=s.residual_var, l=5, num_epochs=10)
    pd.DataFrame(opt['trace'], columns=['elbo', 'llik', 'kl_z', 'kl_b'])
  #+END_SRC

  #+RESULTS: coordinate-ascent-ld
  :RESULTS:
  #+BEGIN_EXAMPLE
           elbo        llik      kl_z       kl_b
    0 -219.908757 -200.251698  8.487061  11.169998
    1 -219.875655 -200.537041  8.216742  11.121872
    2 -220.072682 -200.806115  8.163078  11.103489
    3 -220.145278 -200.903071  8.147945  11.094261
    4 -220.163084 -200.926393  8.144994  11.091697
    5 -220.164622 -200.928121  8.145114  11.091388
    6 -220.163479 -200.926487  8.145488  11.091504
    7 -220.162768 -200.925524  8.145652  11.091592
    8 -220.162557 -200.925248  8.145688  11.091621
    9 -220.162536 -200.925225  8.145687  11.091625
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :ipyfile categorical-slab-geuvadis.png
    plot_categorical_slab_fit(s, opt)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:categorical-slab-geuvadis.png]]
  :END:
